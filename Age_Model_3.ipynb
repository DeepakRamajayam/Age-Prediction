{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ComGhTPN1oku"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEAM_MEMBER_ID = 3\n",
        "\n",
        "print(f\"--- SETUP FOR TEAM MEMBER {TEAM_MEMBER_ID} ---\")\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFyl81Ib1xp8",
        "outputId": "1265d68d-4843-4ed7-c979-7179e8784587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SETUP FOR TEAM MEMBER 3 ---\n",
            "TensorFlow Version: 2.19.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMounting Google Drive...\")\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgWDYIfX11Nm",
        "outputId": "07527df4-3dfb-40cc-c54d-dea7434f8376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_PATH = \"/content/drive/My Drive/\"\n",
        "ZIP_FILE_PATH = os.path.join(DRIVE_PATH, \"final_data.zip\")\n",
        "PARTITION_FILE = os.path.join(DRIVE_PATH, f\"partition_processed_{TEAM_MEMBER_ID}.json\")\n",
        "MODEL_SAVE_PATH = os.path.join(DRIVE_PATH, f\"keras_cnn_model_{TEAM_MEMBER_ID}.keras\")"
      ],
      "metadata": {
        "id": "nPspA5eB13Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nCopying dataset from {ZIP_FILE_PATH}...\")\n",
        "!cp \"{ZIP_FILE_PATH}\" /content/\n",
        "print(\"Unzipping local dataset...\")\n",
        "!unzip -o /content/final_data.zip -d /content/ > /dev/null\n",
        "print(\"Dataset ready on local disk.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R6SlYDo2fpI",
        "outputId": "237af37e-cb5c-4fdc-b9e6-9625297de411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Copying dataset from /content/drive/My Drive/final_data.zip...\n",
            "Unzipping local dataset...\n",
            "Dataset ready on local disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nLoading your unique data partition from: {PARTITION_FILE}\")\n",
        "with open(PARTITION_FILE, 'r') as f:\n",
        "    partition_data = json.load(f)\n",
        "my_files = partition_data['files']\n",
        "my_labels = partition_data['labels']\n",
        "print(f\"Successfully loaded {len(my_files)} unique image paths.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyQh2Kfm2hme",
        "outputId": "e0a04e95-28bb-443c-ae86-8c0d8ea9dc1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading your unique data partition from: /content/drive/My Drive/partition_processed_3.json\n",
            "Successfully loaded 30000 unique image paths.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Building `tf.data` Pipeline ---\")\n",
        "\n",
        "train_files, val_files, train_labels, val_labels = train_test_split(\n",
        "    my_files, my_labels, test_size=0.2, random_state=42, stratify=my_labels\n",
        ")\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def parse_image(filename, label):\n",
        "    image = tf.io.read_file(filename)\n",
        "    image = tf.io.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
        "    image = image / 255.0\n",
        "    return image, label\n",
        "\n",
        "def augment_image(image, label):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    return image, label\n",
        "\n",
        "def create_dataset(files, labels, is_training=True):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((files, labels))\n",
        "    dataset = dataset.map(parse_image, num_parallel_calls=AUTOTUNE)\n",
        "    if is_training:\n",
        "        dataset = dataset.shuffle(buffer_size=1000)\n",
        "        dataset = dataset.map(augment_image, num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_ds = create_dataset(train_files, train_labels)\n",
        "val_ds = create_dataset(val_files, val_labels, is_training=False)\n",
        "print(\"Training and validation datasets are ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmDBhRny2sWG",
        "outputId": "1f5690d6-a158-4540-86d8-24a87dbb8461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Building `tf.data` Pipeline ---\n",
            "Training and validation datasets are ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Defining Model Architecture and Loading Checkpoint if Available ---\")\n",
        "\n",
        "def build_model(input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Dropout(0.25),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Dropout(0.4),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-eX8lpj2vE2",
        "outputId": "d5c93b3c-6dd1-48fd-8a22-f6f788bba121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Defining Model Architecture and Loading Checkpoint if Available ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(MODEL_SAVE_PATH):\n",
        "    print(f\"Checkpoint found! Loading full model from: {MODEL_SAVE_PATH}\")\n",
        "    model = tf.keras.models.load_model(MODEL_SAVE_PATH)\n",
        "else:\n",
        "    print(\"No checkpoint found. Building a new model from scratch.\")\n",
        "    model = build_model()\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss='mae',\n",
        "        metrics=['mae']\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmzTwPlT2yG0",
        "outputId": "4f267587-a8fc-46ba-d434-3ea8916e3aba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found. Building a new model from scratch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Setting Up Training ---\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=MODEL_SAVE_PATH, monitor='val_mae', mode='min',\n",
        "    save_best_only=True, save_weights_only=False, verbose=1\n",
        ")\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_mae', mode='min', patience=5, verbose=1, restore_best_weights=True\n",
        ")\n",
        "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_mae', mode='min', factor=0.2, patience=2, verbose=1\n",
        ")\n",
        "\n",
        "NUM_EPOCHS = 40\n",
        "print(f\"\\nStarting training for Model {TEAM_MEMBER_ID} for up to {NUM_EPOCHS} epochs...\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback, reduce_lr_callback]\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining complete! Your best model has been saved to: {MODEL_SAVE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZl6XsmC20ZR",
        "outputId": "ae36e488-0c7f-4b3d-bfda-d1ab7d14d885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Setting Up Training ---\n",
            "\n",
            "Starting training for Model 3 for up to 40 epochs...\n",
            "Epoch 1/40\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 40.0487 - mae: 40.0487\n",
            "Epoch 1: val_mae improved from inf to 19.27037, saving model to /content/drive/My Drive/keras_cnn_model_3.keras\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 76ms/step - loss: 40.0358 - mae: 40.0358 - val_loss: 19.2704 - val_mae: 19.2704 - learning_rate: 0.0010\n",
            "Epoch 2/40\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 19.8805 - mae: 19.8805\n",
            "Epoch 2: val_mae did not improve from 19.27037\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 72ms/step - loss: 19.8797 - mae: 19.8797 - val_loss: 19.2913 - val_mae: 19.2913 - learning_rate: 0.0010\n",
            "Epoch 3/40\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 18.2787 - mae: 18.2787\n",
            "Epoch 3: val_mae improved from 19.27037 to 17.28441, saving model to /content/drive/My Drive/keras_cnn_model_3.keras\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 74ms/step - loss: 18.2784 - mae: 18.2784 - val_loss: 17.2844 - val_mae: 17.2844 - learning_rate: 0.0010\n",
            "Epoch 4/40\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 17.8564 - mae: 17.8564\n",
            "Epoch 4: val_mae did not improve from 17.28441\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 71ms/step - loss: 17.8562 - mae: 17.8562 - val_loss: 28.3563 - val_mae: 28.3563 - learning_rate: 0.0010\n",
            "Epoch 5/40\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 17.2271 - mae: 17.2271\n",
            "Epoch 5: val_mae did not improve from 17.28441\n",
            "\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 72ms/step - loss: 17.2272 - mae: 17.2272 - val_loss: 18.8348 - val_mae: 18.8348 - learning_rate: 0.0010\n",
            "Epoch 6/40\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 16.7159 - mae: 16.7159\n",
            "Epoch 6: val_mae improved from 17.28441 to 15.58188, saving model to /content/drive/My Drive/keras_cnn_model_3.keras\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 74ms/step - loss: 16.7157 - mae: 16.7157 - val_loss: 15.5819 - val_mae: 15.5819 - learning_rate: 2.0000e-04\n",
            "Epoch 7/40\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 16.2453 - mae: 16.2453\n",
            "Epoch 7: val_mae did not improve from 15.58188\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 70ms/step - loss: 16.2453 - mae: 16.2453 - val_loss: 16.5667 - val_mae: 16.5667 - learning_rate: 2.0000e-04\n",
            "Epoch 8/40\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 15.8969 - mae: 15.8969\n",
            "Epoch 8: val_mae improved from 15.58188 to 15.28889, saving model to /content/drive/My Drive/keras_cnn_model_3.keras\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 73ms/step - loss: 15.8967 - mae: 15.8967 - val_loss: 15.2889 - val_mae: 15.2889 - learning_rate: 2.0000e-04\n",
            "Epoch 9/40\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 15.6400 - mae: 15.6400\n",
            "Epoch 9: val_mae improved from 15.28889 to 14.76675, saving model to /content/drive/My Drive/keras_cnn_model_3.keras\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 75ms/step - loss: 15.6399 - mae: 15.6399 - val_loss: 14.7668 - val_mae: 14.7668 - learning_rate: 2.0000e-04\n",
            "Epoch 10/40\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 15.4747 - mae: 15.4747\n",
            "Epoch 10: val_mae did not improve from 14.76675\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 69ms/step - loss: 15.4747 - mae: 15.4747 - val_loss: 17.4080 - val_mae: 17.4080 - learning_rate: 2.0000e-04\n",
            "Epoch 11/40\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 15.3306 - mae: 15.3306\n",
            "Epoch 11: val_mae did not improve from 14.76675\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 69ms/step - loss: 15.3306 - mae: 15.3306 - val_loss: 14.8543 - val_mae: 14.8543 - learning_rate: 2.0000e-04\n",
            "Epoch 12/40\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 15.3730 - mae: 15.3730\n",
            "Epoch 12: val_mae did not improve from 14.76675\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 72ms/step - loss: 15.3728 - mae: 15.3728 - val_loss: 15.3154 - val_mae: 15.3154 - learning_rate: 4.0000e-05\n",
            "Epoch 13/40\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 14.9853 - mae: 14.9853\n",
            "Epoch 13: val_mae did not improve from 14.76675\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 71ms/step - loss: 14.9853 - mae: 14.9853 - val_loss: 14.8670 - val_mae: 14.8670 - learning_rate: 4.0000e-05\n",
            "Epoch 14/40\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 14.9564 - mae: 14.9564\n",
            "Epoch 14: val_mae did not improve from 14.76675\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 72ms/step - loss: 14.9564 - mae: 14.9564 - val_loss: 15.3674 - val_mae: 15.3674 - learning_rate: 8.0000e-06\n",
            "Epoch 14: early stopping\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "\n",
            "Training complete! Your best model has been saved to: /content/drive/My Drive/keras_cnn_model_3.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MvNXLCVI8O3j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}